---
title: "L1"
author: "COMPASS workflows team"
title-block-banner: true
params:
  html_outfile: "L1.html"
  DATA_ROOT: "data_TEST/"
  L1_NORMALIZE: "L1_normalize/"
  L1: "L1/"
  METADATA_ROOT: "metadata/"
  METADATA_SITE_FILES: "site_files/"
  QAQC_TABLE: "qaqc_table.csv"
  RELEASE_README_FILES: "readme_files/"
  L1_METADATA: "L1_metadata/"
  METADATA_VARS_TABLE: "variables_metadata.csv"
  METADATA_COLUMNS_TABLE: "L1_metadata_columns.csv"
  # We use "Etc/GMT+5" rather than e.g. "America/New_York" for
  # L1_DATA_TIMEZONE because outputs should always be in STANDARD time
  # See https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
  L1_DATA_TIMEZONE: "Etc/GMT+5"
  CODE_EXAMPLES: "code_examples/"
  L1_VERSION: "???"
  L1_RELEASE_DATE: "???"
  write_plots: true
  logfile: ""
  run_parallel: true
date: now
date-format: "YYYY-MM-DD HH:mm:ssZ"
format: 
  html:
    embed-resources: true
    code-fold: true
editor: visual
---

This script

-   Reads in all the L1_normalize files

-   Compiles and writes them out into separate \<site\>\<year\>\<month\> files

-   Writes a metadata file for each folder

## Initializing

```{r init}
#| include: false

library(ggplot2)
theme_set(theme_bw())

# Warnings are not allowed here
oldwarn <- options()$warn
options(warn = 2)

source("helpers.R")
source("L1-utils.R")
source("metadata-utils.R")
start_timer("everything")

# Get the column metadata file
column_md <- read.csv(file.path(params$METADATA_ROOT,
                                params$L1_METADATA,
                                params$METADATA_COLUMNS_TABLE),
                      comment.char = "#",
                      stringsAsFactors = FALSE)

L1_NORMALIZE <- file.path(params$DATA_ROOT, params$L1_NORMALIZE)
dirs_to_process <- scan_folders(L1_NORMALIZE)
#dirs_to_process <- dirs_to_process[grep("CRC_UP", names(dirs_to_process))] # TESTING ONLY

# Get the QAQC table with outlier information
QAQC_TABLE <- file.path(params$METADATA_ROOT, params$QAQC_TABLE)
qaqct <- read_csv(QAQC_TABLE, col_types = "cccccc")
qaqct <- qaqct[!is.na(qaqct$variable),]

# Get the variable metadata
L1_VAR_MD <- read.csv(file.path(params$METADATA_ROOT, 
                                params$METADATA_VARS_TABLE))
L1_VAR_MD <- L1_VAR_MD[L1_VAR_MD$research_name != "",]

L1 <- file.path(params$DATA_ROOT, params$L1)
```

I see `r length(dirs_to_process)` directories to process in `r L1_NORMALIZE`.

QAQC table `r QAQC_TABLE` and has `r nrow(qaqct)` entries.

L1 column metadata has `r nrow(column_md)` entries.

Output directory is `r L1`.

HTML outfile is "`r params$html_outfile`".

## Processing

```{r processing}

f <- function(dir_name, dirs_to_process, out_dir) {
    message(Sys.time(), " Processing ", basename(dir_name))
    # cat(as.character(Sys.time()), " Doing ", basename(dir_name), "\n", 
    #     file = "~/Desktop/log.txt", append = T)
    d <- dirs_to_process[[dir_name]]
    message("\tIt has ", length(d), " files")
    
    # Read all files in a folder
    start_timer("reading-data")
    dat_raw <- read_csv_group(d, quiet = TRUE, col_types = "cccccTdccccccccdii")
    stop_timer("reading-data")
    
    message("\tTotal data: ", nrow(dat_raw), " rows, ", ncol(dat_raw), " columns")
    
    # Remove duplicate rows (e.g. from multiple datalogger downloads)
    dat <- dat_raw[!duplicated(dat_raw), ]
    if(nrow(dat_raw) - nrow(dat) > 0) {
        message("\tRemoved ", nrow(dat_raw) - nrow(dat), " duplicate rows")
    }
    
    # TODO: there might be more than one instrument per file (battery voltage)
    
    start_timer("outliers")
    rn <- unique(dat$research_name)
    if(length(rn) > 1) {
        stop("Why is there more than one research name here?!?")
    }
    i <- match(rn, qaqct$variable)
    if(is.na(i)) stop(rn, " not found in QAQC table")
    if(length(i) > 1) stop(rn, " found in more than one row of QAQC table")

    algo <- qaqct$outlier_detection[i]
    dat$F_MAD <- NA
    n_outlier <- NA
    if(is.na(algo)) {
        message("\tNo outlier detection")
    } else {
        otherparams <- qaqct$params[i]
        message("\tRunning outlier algorithm ", algo, otherparams)
        dat <- do_outlier_test(dat, 
                               algo, 
                               time_grouping = qaqct$time_grouping[i], 
                               otherparams = otherparams)
        n_outlier <- sum(dat$F_MAD, na.rm = TRUE)
        if(n_outlier > 0) {
            message("\t\t", n_outlier, " outliers identified")
        }
    }
    stop_timer("outliers")
    
    # File-based summary
    smry <- data.frame(Dir = dir_name, 
                       Files = length(d), 
                       Rows = nrow(dat),
                       Outliers = n_outlier,
                       NA_rows = sum(is.na(dat$Value)))
    
    if(nrow(dat)) {
        site <- dat$Site[1]
        plot <- dat$Plot[1]
        
        dat <- check_drop_sort_columns(dat, column_md, params$METADATA_COLUMNS_TABLE)
        
        start_timer("writing-data")
        write_to_folders(dat, 
                         root_dir = out_dir, 
                         data_level = "L1",
                         site = site,
                         plot = plot,
                         variable_metadata = L1_VAR_MD,
                         version = params$L1_VERSION,
                         write_plots = params$write_plots)
       stop_timer("writing-data")
    }
    return(smry)
}

# We can optionally process in parallel using all available cores
if(params$run_parallel) {
    library(parallel)
    apply_func = mclapply
    # L1 is memory-intensive, especially for the TEMPEST files, and 
    # it's possible to run out of system memory, so limit cores used
    options(mc.cores = min(4, detectCores()))
    message("Using parallel::mclapply with ", getOption("mc.cores"), " cores")
} else {
    apply_func = lapply
}

log_info("About to L1", logfile = params$logfile)
out <- apply_func(names(dirs_to_process),
                  f, 
                  dirs_to_process = dirs_to_process, 
                  out_dir = L1)

# Check whether any of the parallel jobs errored
if(params$run_parallel && "try-error" %in% sapply(out, class)) {
    stop("Error! Rerun non-parallel to diagnose")
}
start_timer("proc-cleanup")
out_df <- do.call("rbind", out)

if(!is.numeric(out_df$Rows)) {
    x <- which(is.na(as.numeric(out_df$Rows)))[1]
    stop("Rows is not numeric: ", out_df$Dir[x], ",",
         out_df$Files[x], ",",
         out_df$Rows[x])
}

n_obs <- format(sum(out_df$Rows), big.mark = ",")
n_na <- format(sum(out_df$NA_rows), big.mark = ",")
stop_timer("proc-cleanup")

# Restore old warning setting
options(warn = oldwarn)
```

## Metadata

L1 metadata template directory is `r params$L1_METADATA`.

```{r metadata}
# Write the overall README
start_timer("metadata")
readme_fn <- file.path(params$METADATA_ROOT,
                       params$L1_METADATA,
                       params$RELEASE_README_FILES, 
                       paste0("README_v", params$L1_VERSION, ".txt"))
readme <- md_readme_substitutions(readme_fn,
                                  params$L1_VERSION, 
                                  params$L1_RELEASE_DATE, 
                                  n_obs,
                                  GIT_COMMIT,
                                  params$L1_DATA_TIMEZONE)
readme_outfn <- file.path(L1, basename(readme_fn))
message("Writing overall README ", readme_outfn, "...")
writeLines(readme, readme_outfn)

# Get the L1 template file
template_file <- file.path(params$METADATA_ROOT,
                           params$L1_METADATA, 
                           "L1_metadata_template.txt")
if(!file.exists(template_file)) {
    stop("Couldn't find file ", basename(template_file), " in ", params$L1_METADATA)
}
L1_metadata_template <- readLines(template_file)

col_md_for_insert <- paste(sprintf("%-15s", column_md$Column), column_md$Description)

# Format the variable metadata
var_md_for_insert <- md_variable_info(L1_VAR_MD, L1_or_L2 = "L1")

message("Main template has ", length(L1_metadata_template), " lines")
message("Column metadata info has ", length(col_md_for_insert), " lines")
message("Variable metadata info has ", length(var_md_for_insert), " lines")

# Identify the main data directories in L1, which are <site>_<year>
data_dirs <- list.files(L1, pattern = "^[a-zA-Z]+_[0-9]{4}$")
site_files_folder <- file.path(params$METADATA_ROOT, 
                              params$METADATA_SITE_FILES)

for(dd in data_dirs) {
    dd_full <- file.path(L1, dd)
    message("Generating metadata for ", dd_full)
    
    message("\tInserting timestamp and folder name")
    md <- gsub("[TIMESTAMP]", date(), L1_metadata_template, fixed = TRUE)
    md <- gsub("[FOLDER_NAME]", dd, md, fixed = TRUE)
    
    # Insert info on data files into metadata
    md <- md_insert_fileinfo(dd_full, md, level = "L1")
   
    # Insert column metadata
    col_info_pos <- grep("[COLUMN_INFO]", md, fixed = TRUE)
    md <- append(md, col_md_for_insert, after = col_info_pos)
    md <- md[-col_info_pos]
    
    # Insert NA code, time zone, and version information
    md <- md_insert_miscellany(md, 
                               NA_STRING_L1, 
                               params$L1_DATA_TIMEZONE, 
                               params$L1_VERSION)
    
    # Insert variable metadata
    var_info_pos <- grep("[VARIABLE_INFO]", md, fixed = TRUE)
    md <- append(md, var_md_for_insert, after = var_info_pos)
    md <- md[-var_info_pos]
    
    # Site information
    # Folders are <site>_<year>
    site <- strsplit(dd, "_")[[1]][1]
    md <- md_insert_siteinfo(site, site_files_folder, md)

    # QAQC information
    qaqct <- qaqct[!is.na(qaqct$outlier_detection),]
    qaqc_for_insert <- paste(sprintf("%-20s", c("Variable", qaqct$variable)),
                           sprintf("%-15s", c("Time_grouping", qaqct$time_grouping)),
                           sprintf("%-15s", c("Extra_params", qaqct$params)),
                           sprintf("%-20s", c("Notes", qaqct$Notes)))
    qaqc_for_insert <- gsub("NA", "--", qaqc_for_insert) # looks nicer
    qaqc_info_pos <- grep("[OUTLIER_INFO]", md, fixed = TRUE)
    md <- append(md, qaqc_for_insert, after = qaqc_info_pos)
    md <- md[-qaqc_info_pos]

    # Write the final metadata file
    mdfn <- paste0(dd, "_L1_v", params$L1_VERSION, "_metadata.txt")
    message("\tWriting ", mdfn, "...")
    writeLines(md, file.path(L1, dd, mdfn))
}
stop_timer("metadata")
```

## Documentation and code

```{r docs-and-code}
start_timer("docs-code")
ok <- file.copy(params$CODE_EXAMPLES, L1, recursive = TRUE)
if(ok) {
    message("Copied ", params$CODE_EXAMPLES, " to ", L1)
} else {
    warning("There was a problem copying ", params$CODE_EXAMPLES)
}
stop_timer("docs-code")
```

## Output summary

```{r output_summary_table}
knitr::kable(out_df)
```

Total rows written: `r n_obs`

Total NA rows written: `r n_na`

## Clean up

```{r cleanup}
#fdb_cleanup() # Close flags database
stop_timer("everything")

print(timer_tracker)
```

## Reproducibility

Git commit `r GIT_COMMIT`.

```{r reproducibility}
sessionInfo()
```
