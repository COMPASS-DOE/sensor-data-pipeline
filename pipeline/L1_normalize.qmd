---
title: "L1_normalize"
author: "COMPASS workflows team"
title-block-banner: true
params:
  html_outfile: "L1_normalize.html"
  DATA_ROOT: "data_TEST/"
  L0: "L0/"
  L1_NORMALIZE: "L1_normalize/"
  DESIGN_TABLE: "design_table.csv"
  # We use "Etc/GMT+5" rather than e.g. "America/New_York" for
  # L1_DATA_TIMEZONE because outputs should always be in STANDARD time
  # See https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
  L1_DATA_TIMEZONE: "Etc/GMT+5"
  METADATA_ROOT: "metadata/"
  METADATA_TIMEZONES_TABLE: "L1_metadata/L1_metadata_timezones.csv"
  METADATA_VARS_TABLE: "variables_metadata.csv"
  OOS: "out-of-service/"
  logfile: ""
  run_parallel: false
date: now
date-format: "YYYY-MM-DD HH:mm:ssZ"
format: 
  html:
    embed-resources: true
    code-fold: true
editor: visual
---

This script

-   Reads in the L0 files one by one

-   Joins with the design table, adding `Site`, `design_link`, and `research_name` columns (every loggernet variable must have an entry)

-   Performs unit conversion (every research name must have an entry)

-   Performs bounds checking (adding a `F_OOB` flag) for each `research_name` variable

-   Integrated out-of-service records into a `F_OOS` flag

-   Writes into folders named with site, plot, year, month

## Initializing

```{r init}
#| include: false

library(lubridate)
library(tidyr)
library(readr)
library(dplyr)
source("L1_normalize-utils.R")

# Warnings are not allowed here, as this usually means a column format
# problem that we want to fix immediately
oldwarn <- options()$warn
options(warn = 2)

# Read the design table (everything must have an entry)
DESIGN_TABLE <- file.path(params$METADATA_ROOT, params$DESIGN_TABLE)
dt <- read_csv(DESIGN_TABLE, col_types = "cccccccccDcc")
dt$note <- NULL
dt <- dt[!is.na(dt$Logger),] # remove empty rows

# For compactness, the design table may have expansions. For example,
# "DiffVoltA_Avg({1:8})" -> "DiffVoltA_Avg(1)", "DiffVoltA_Avg(2)", etc.
# Expand these rows into their individual entries
dt_ex <- expand_df(dt)

# The Site-Plot-instrument-which-individual-research_name columns
# should be unique (for rows with non-empty research_name entries)
dt_ex$design_link <- with(dt_ex, paste(Site, Plot, Instrument, Instrument_ID, 
                                       Sensor_ID, research_name, sep = "-"))
chk <- dt_ex[!is.na(dt_ex$research_name), "design_link"]
if(any(duplicated(chk))) {
    stop("There are duplicate mappings in the design table! ",
         paste(head(chk[duplicated(chk), "design_link"]), collapse = ", "))
}
DESIGN_TABLE_ENTRIES <- with(dt_ex, paste(Logger, Table, loggernet_variable)) # save for later

# Read the site time zone table
METADATA_TZ_TABLE <- file.path(params$METADATA_ROOT, params$METADATA_TIMEZONES_TABLE)
tzt <- read_csv(METADATA_TZ_TABLE, col_types = "ccc")

# Read the variable metadata table
METADATA_VARS_TABLE <- file.path(params$METADATA_ROOT, params$METADATA_VARS_TABLE)
mt <- read_csv(METADATA_VARS_TABLE, col_types = "ccccccddc")
# ...and create the units table (everything must have an entry)
ut <- mt[!is.na(mt$research_name),] # filter
ut$new_unit = ut$final_units # rename
ut <- ut[c("research_name", "conversion", "new_unit")] # select

# Create the bounds table (not everything needs an entry)
bt <- mt[!is.na(mt$research_name) & !is.na(mt$final_units),] # filter
bt$units = bt$final_units # rename
bt <- bt[c("research_name", "units", "low_bound", "high_bound")] # select

L0 <- file.path(params$DATA_ROOT, params$L0)
files_to_process <- list.files(L0, pattern = "*.csv$", full.names = TRUE)

L1_NORMALIZE <- file.path(params$DATA_ROOT, params$L1_NORMALIZE)

source("helpers.R")

# Read out-of-service data and set up a structure to keep track
# of whether each table is used or not
oos_data <- read_oos_data(params$OOS)
oos_data_used <- rep(FALSE, length.out = length(oos_data))
names(oos_data_used) <- names(oos_data)

# Restore old warning setting
options(warn = oldwarn)
```

I see `r length(files_to_process)` files to process in `r L0`.

Output directory is `r L1_NORMALIZE`.

Design table "`r DESIGN_TABLE`" and has `r nrow(dt)` rows, `r nrow(dt_ex)` after expansion.

Variable metadata table is "`r METADATA_VARS_TABLE`" and has `r nrow(mt)` rows.

Units table has `r nrow(ut)` rows; bounds table `r nrow(bt)`.

There are `r length(oos_data)` out-of-service tables to pay attention to.

HTML outfile is "`r params$html_outfile`".

## Processing

```{r processing}
overwrites <- 0
errors <- 0
out <- list()

out_dir <- L1_NORMALIZE

for(i in seq_along(files_to_process)) {
    
    fn <- files_to_process[i]
    message(Sys.time(), " ", i, "/", length(files_to_process), " Processing ", basename(fn))
    
    # The row of the summary data frame, displayed at the end of processing, for this data file
    smry <- data.frame(File = basename(fn), 
                       no_research_name = NA_integer_,
                       `OOB%` = NA_real_,
                       `OOS%` = NA_real_,
                       Note = "",
                       check.names = FALSE)
    dat <- read_csv(fn, 
                    # don't want timestamp parsed to a datetime at this point
                    col_types = list(TIMESTAMP = col_character()),
                    na = c("NA", "NAN", "NaN", ""))

    # ------------- Design table
    
    # Check for missing entries in the design table
    ltlv <- unique(paste(dat$Logger, dat$Table, dat$loggernet_variable))
    present <- ltlv %in% DESIGN_TABLE_ENTRIES
    if(!all(present)) {
        stop("Some data file entries are missing in the design table!\n",
             paste(ltlv[!present], collapse = ", "))
    }

    # Join with design table
    old_rows <- nrow(dat)
    dat <- left_join(dat, dt_ex,
                 by = c("Logger", "Table", "loggernet_variable"))
    # This is a left join, and normally should not have changed the number of rows
    # The exception would be if a sensor has been reassigned; in that case it will have
    # >1 entry in the design table, with the "valid_through" column controlling when the
    # old assignment becomes invalid and the new one takes over. Call valid_entries()
    # (in helpers.R) to figure out which mappings should apply.
    message("\tChecking for multiple-match design links...")
    dat_retain <- valid_entries(objects = dat$loggernet_variable,
                                times = substr(dat$TIMESTAMP, 1, 10), # isolate YYYY-MM-DD
                                valid_through = dat$valid_through)

    if(sum(!dat_retain)) message("\tDropping ", sum(!dat_retain), " out-of-date design links")
    dat <- dat[dat_retain,]
    dat$valid_through <- NULL
    
    # At this point, there should be exactly one match for every loggernet variable
    if(nrow(dat) > old_rows) {
        counts <- aggregate(design_link ~ loggernet_variable, data = dat, 
                            FUN = function(x) length(unique(x)))
        counts <- counts[counts$design_link > 1,]
        stop("Some loggernet variables in ", fn, " have more than one design_link: ",
             paste(counts$loggernet_variable, collapse = ", "))
    }
    
    # Summary information
    smry$no_research_name <- sum(is.na(dat$research_name))
    message("\tFiltering out ", smry$no_research_name, " empty research_name rows")
    dat <- subset(dat, !is.na(dat$research_name))
 
    # Remove empty data
    dat <- dat[!is.na(dat$value), ]

    # If no rows left, note this fact and move on
    if(!nrow(dat)) {
        message("\tNo research_names left; nothing to process")
        smry$Note <- "No research_names left; nothing to process"
        next
    }
    
    # ------------- Unit conversion
    
    # Check for missing entries in the units table
    rns <- unique(dat$research_name)
    missings <- rns[!rns %in% ut$research_name]
    if(length(missings)) {
        missing_units <- paste(missings, collapse = ", ")
        missing_units <- strwrap(missing_units, width = 40)
        stop("\tERROR: missing units entries: ",
                paste(missing_units, collapse = "\n\t\t"))
    }
    
    nums <- can_convert_to_numeric(dat$value)
    message("\tDoing unit conversion (for ", sum(nums), "/", nrow(dat), " numeric)")
    # The value column may have both numeric and character entries
    # (e.g., time of max temperature); identify and run unit conversion
    # and out-of-bounds checking on the former.
    value_nums <- as.numeric(dat$value[nums])
    uc_out <- unit_conversion(value_nums, dat$research_name[nums], ut)

    # Rename value to value_raw; copy to Value; overwrite numeric
    # entries with converted values and add corresponding units column
    names(dat)[names(dat) == "value"] <- "value_raw"
    dat$Value <- dat$value_raw
    dat$Value[nums] <- as.character(uc_out$value_conv)
    dat$units <- ""
    dat$units[nums] <- uc_out$units

    # ------------- Out-of-bounds flags
    
    message("\tAdding OOB flags")
    dat <- left_join(dat, bt, by = c("research_name", "units"))
    dat$F_OOB <- NA_integer_
    vals <- as.numeric(dat$Value[nums])
    dat$F_OOB[nums] <- as.integer(
        vals < dat$low_bound[nums] | vals > dat$high_bound[nums])
    smry$`OOB%` <- round(sum(dat$F_OOB[nums]) / length(nums) * 100, 1)
    dat$low_bound <- dat$high_bound <- NULL

    # ------------- Out-of-service flags

    # The out of service check is very expensive, so only do
    # it when the `Table` of the current data is in our list 
    # of OOS tables to process
    dat$F_OOS <- 0L
    for(tbl in names(oos_data)) {
        if(grepl(pattern = tbl, dat$Table[1], fixed = TRUE)) {
            # This assumes there's only one entry in `oos_data` per 
            # table type, e.g. "ExoTable" or "WaterLevel600"
            # Is this reasonable? If not, we need to run through
            # all `oos_data` entries, OR'ing the results as we go, 
            # instead of breaking out
            dat$F_OOS <- as.integer(oos(oos_data[[tbl]], dat))
            if(sum(dat$F_OOS)) message("\tAdded ", sum(dat$F_OOS), " OOS flags for ", tbl)
            oos_data_used[tbl] <- TRUE # won't work when parallelized
            break
        }
    }
    smry$`OOS%` <- round(sum(dat$F_OOS) / nrow(dat) * 100, 1)

    # ------------- Time zone change, if needed
    
    site <- dat$Site[1]
    if(site %in% tzt$Site) {
        site_tz <- tzt$Datalogger_time_zone[which(site == tzt$Site)]
        if(site_tz != params$L1_DATA_TIMEZONE) {
            message("\tChanging timestamps from ", site_tz, " to ", params$L1_DATA_TIMEZONE)
            # The timestamps were recorded in the site's time zone, which differs
            # from the L1 timezone, so convert to POSIXct and change tz
            timestamps <- ymd_hms(dat$TIMESTAMP, tz = site_tz)
            timestamps <- with_tz(timestamps, tzone = params$L1_DATA_TIMEZONE)
            dat$TIMESTAMP <- format(timestamps, "%Y-%m-%d %H:%M:%S")
        }
    } else {
        stop("This site ", site, " is not in the time zone table ", 
             params$METADATA_TIMEZONES_TABLE)
    }
    
    # ------------- Write output files and clean up
    
    # It is possible for a datalogger file to have assignments to more 
    # than one site or plot :( because some sapflow sensors in CRC-UP were
    # reassigned in June 2025 to DELUGE.
    # Since this is only an issue for L1_normalize, we handle it here and
    # keep write_to_folders() blissfully ignorant
    dat_split <- split(dat, paste(dat$Site, dat$Plot, dat$Logger, dat$Table))
    for(ds in dat_split) {
        write_to_folders(ds,
                         root_dir = out_dir,
                         data_level = "L1_normalize",
                         site = ds$Site[1],
                         plot = ds$Plot[1],
                         logger = ds$Logger[1],
                         table = ds$Table[1])
    }
    
    # The Chesapeake Bay synoptic site design overlaps with TEMPEST, and
    # the latter's control ("C") plot functions as the former's upland
    # plot. So if we're handling TMP C data, also write a copy as GCW
    # UP. https://github.com/COMPASS-DOE/sensor-data-pipeline/issues/116
    if(dat$Site[1] == "TMP" && dat$Plot[1] == "C") {
        dat$Site <- "GCW"
        dat$Plot <- "UP"
        message("\t--> DUPLICATING DATA TO GCW UP <--")
        write_to_folders(dat, 
                         root_dir = out_dir, 
                         data_level = "L1_normalize",
                         site = dat$Site[1],
                         plot = dat$Plot[1],
                         logger = dat$Logger[1],
                         table = dat$Table[1])
    }

    out[[i]] <- smry
}

# Check whether all out-of-service tables were checked
# If not, this probably indicates a naming mistake
if(!params$run_parallel && !all(oos_data_used)) {
    warning("Out-of-service tables never used: ",
         paste(names(oos_data_used)[!oos_data_used], collapse = ","))
}
```

## Summary

```{r summary}
#| echo: false
#| output: asis
if(overwrites) {
    cat("### WARNING: ", overwrites, " file overwrite(s)\n")
    log_warning(paste("File overwrite(s)", params$html_outfile), 
                logfile = params$logfile)
}
if(errors) {
    cat("### ERROR: ", errors, " error(s)\n")
    log_warning(paste("File read/write or processing error(s)", params$html_outfile), 
                logfile = params$logfile)
}
```

```{r summary_table}
out_df <- do.call("rbind", out)
knitr::kable(out_df)
```

## Reproducibility

Git commit `r GIT_COMMIT`.

```{r reproducibility}
sessionInfo()
```
