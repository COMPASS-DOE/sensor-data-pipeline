---
title: "Workflow: L0"
author: "COMPASS workflows team"
title-block-banner: true
params:
  html_outfile: "L0.html"
  DATA_ROOT: "data_TEST/"
  RAW: "Raw/"
  RAW_REMOVED: "removed_raw_files.txt"
  RAW_ORIGINAL: "Raw_original"
  RAW_EDITED: "Raw_edited"
  L0: "L0/"
  SKIP_IF_EXISTS: true
  logfile: ""
  run_parallel: false
date: now
date-format: "YYYY-MM-DD HH:mm:ssZ"
format: 
  html:
    embed-resources: true
    code-fold: true
editor: visual
---

This script

-   Reads in the raw data files one by one

-   By default skip a raw file if a corresponding L0 file already exists

-   Checks whether the file is on a 'removal list', and/or whether it should be replaced by an edited (corrected) version

-   Extracts `Logger` and `Table` information from the header and adds them as columns

-   Reshapes from wide to long; only `Logger`, `Table`, and `TIMESTAMP` don't get reshaped

-   Adds a source file ID (using `digest::digest()` )

-   Writes as CSV files with raw file hash info in filename

## Initializing

```{r init}
#| include: false

raw_root <- file.path(params$DATA_ROOT, params$RAW)
RAW <- file.path(raw_root, params$RAW_ORIGINAL)
L0 <- file.path(params$DATA_ROOT, params$L0)
removal_list <- readLines(file.path(raw_root, params$RAW_REMOVED))
edit_list <- list.files(file.path(raw_root, params$RAW_EDITED), pattern = "dat$")

library(tidyr)
library(readr)

# Warnings are not allowed here, as this usually means a file
# problem that we want to fix immediately
oldwarn <- options()$warn
options(warn = 2)

files_to_process <- list.files(RAW, pattern = "*.dat$", 
                               full.names = TRUE, recursive = TRUE)

source("helpers.R")
source("L0-utils.R")
```

I see `r length(files_to_process)` files to process in `r RAW`.

The removal list has `r length(removal_list)` entries. The edit list has `r length(edit_list)` entries.

Output directory is `r L0`.

HTML outfile is `r params$html_outfile`.

Logfile is `r params$logfile`.

Working directory is `r getwd()`.

## Processing

```{r processing}
errors <- 0

time_reading <- 0
time_reshaping <- 0
time_writing <- 0

f <- function(fn, new_dir) {

    basefn <- basename(fn)
    message(Sys.time(), " Processing ", basefn)
    note <- ""

    # Check whether this file is on the edit list, and
    # if yes, read from the edited directory
    if(any(grepl(basefn, edit_list))) {
        message("\tFile is on the edit list")
        fn <- file.path(raw_root, params$RAW_EDITED, basefn)
        note <- "Used edited version"
    }

    short_hash <- substr(digest::digest(file = fn, algo = "md5"), 1, 8)

    # Check whether this file is on the removal list
    if(any(grepl(basefn, removal_list))) {
        message("\tFile is on the removal list; skipping")
        return(data.frame(File = basefn,
               Rows = NA,
               Columns = NA,
               Raw_hash = short_hash,
               Note = "On removal list; skipped"))
    }
    
    # Construct the new filename and check if L0 file exists
    # The new filename is old filename + hash of dat file
    # This is so we can distinguish datasets with identical raw filenames
    # but differing data contents (can happen sometimes with dataloggers)
    stripped_fn <- gsub("\\.dat$", "", basefn)
    new_fn <- paste0(stripped_fn, "_", short_hash, ".csv")
    new_fqfn <- file.path(new_dir, new_fn)
    if(params$SKIP_IF_EXISTS && file.exists(new_fqfn)) {
        message("\tL0 file already exists; skipping")
        return(data.frame(File = basefn,
               Rows = NA,
               Columns = NA,
               Raw_hash = short_hash,
               Note = "L0 exists; skipped"))
    }

    # Read the raw data. This is an expensive step
    x <- system.time({
        dat <- read_datalogger_file(fn, col_types = cols(.default = col_character()))
    })
    time_reading <<- time_reading + x[3]
    
    message("\tOriginal data is ", nrow(dat), " x ", ncol(dat))
    long_rows <- long_cols <- NA

    # read_datalogger_file() adds a "Table" column giving the name of the 
    # datalogger table. If this ends with "_1min" or "_5min" then strip that
    # out; this refers to the temporary tables enabled during the TEMPEST floods.
    # We want these processed as normal data, just on a finer time resolution.
    PATTERN <- "_[0-9]{1}min"
    if(grepl(PATTERN, dat$Table[1])) {
        message("\tRemoving _{num}min from table name: ", dat$Table[1])
        dat$Table <- gsub(PATTERN, "", dat$Table)
    }
    
    # Roy: "I generally have a filter that checks and throws out data with very few lines 
    # as they are usually garbage, I have my input check for <4 lines after header" 
    if(nrow(dat) < 4) {
        message("\tRaw data file is very short and likely garbage; skipping")
        note <- "Too short; skipped"
    } else {
        # Pivot to long form. We need to do this now to calculate unique observation IDs
        x <- system.time({
            dat_long <- pivot_longer(dat, c(-Logger, -Table, -TIMESTAMP),
                                     names_to = "loggernet_variable",
                                     values_to = "value")
        })
        time_reshaping <<- time_reshaping + x[3]

        long_rows <- nrow(dat_long)
        long_cols <- ncol(dat_long)
        message("\tPivoted data is ", long_rows, " x ", long_cols)
        
        # Remove duplicates
        dupes <- duplicated(dat_long)
        if(any(dupes)) {
            message("\rRemoving ", sum(dupes), " duplicate rows")
            dat_long <- dat_long[!dupes,]
        }
        
        dat_long$Source_file <- short_hash
        
        # Write the new file
        message("\tWriting ", new_fqfn)
        x <- system.time({
            readr::write_csv(dat_long, new_fqfn)
        })
        time_writing <<- time_writing + x[3]
        rm(dat_long)
    }
    
    # Return an informational data frame about file processed, dimensions, etc.
    data.frame(File = basefn,
               Rows = long_rows,
               Columns = long_cols,
               Raw_hash = short_hash,
               Note = note)
}

# We can optionally process in parallel
if(params$run_parallel) {
    library(parallel)
    apply_func = mclapply
    options(mc.cores = max(detectCores() - 2, 1))
    message("Using parallel::mclapply with ", getOption("mc.cores"), " cores")
} else {
    apply_func = lapply
}

log_info("About to L0", logfile = params$logfile)
out <- apply_func(files_to_process, f, new_dir = L0)

# Check whether any of the parallel jobs errored
if(params$run_parallel && "try-error" %in% sapply(out, class)) {
    stop("Error! Rerun non-parallel to diagnose")
}
```

## Summary

```{r summary}
#| echo: false
#| output: asis
if(errors) {
    cat("#### WARNING: ", errors, " file read/write error(s)\n")
    log_warning(paste("File read/write error(s)", params$html_outfile), 
                logfile = params$logfile)
}

time_total <- time_reading + time_reshaping + time_writing
```

Time reading = `r time_reading` (`r round(time_reading / time_total * 100, 0)`%)

Time reshaping = `r time_reshaping` (`r round(time_reshaping / time_total * 100, 0)`%)

Time writing = `r time_writing` (`r round(time_writing / time_total * 100, 0)`%)

Total time = `r time_total`

```{r summary_table}
out_df <- do.call("rbind", out)
knitr::kable(out_df)
```

## Reproducibility

Git commit `r GIT_COMMIT`.

```{r reproducibility}
sessionInfo()

# Restore old warning setting
options(warn = oldwarn)
```
