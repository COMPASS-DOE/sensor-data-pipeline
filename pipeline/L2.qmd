---
title: "L2"
author: "COMPASS workflows team"
title-block-banner: true
params:
  html_outfile: "L2.html"
  DATA_ROOT: "data_TEST/"
  L2: "L2/"
  L2_QAQC: "L2_qaqc/"
  QAQC_TABLE: "qaqc_table.csv"
  timestamp_round: "15 min"
  METADATA_ROOT: "metadata/"
  METADATA_SITE_FILES: "site_files/"
  METADATA_VARS_TABLE: "variables_metadata.csv"
  METADATA_COLUMNS_TABLE: "L2_metadata_columns.csv"
  RELEASE_README_FILES: "readme_files/"
  L2_METADATA: "L2_metadata/"
  DERIVED_VARS_TABLE: "derived_variables.csv"
  L2_RELEASE_DATE: "???"
  # L2_RELEASE_DATE is YYYY-MM-DD
  L2_DATA_TIMEZONE: "Etc/GMT+5"
  L2_VERSION: "???"
  # L2_VERSION should not have "v" in it; e.g. "2-1"
  CODE_EXAMPLES: "code_examples_L2/"
  run_parallel: false
  write_plots: true
  logfile: ""
date: now
date-format: "YYYY-MM-DD HH:mm:ssZ"
format: 
  html:
    embed-resources: true
    code-fold: true
editor: visual
---

This script

-   Reads in the L2_qaqc data files one by one

-   Computes derived variables

-   Calls gap-filling algorithms (currently only mean annual cycle)

-   Writes L2 files and accompanying documentation

## Initializing

```{r init}
#| include: false

library(dplyr)
library(tidyr)
library(readr)
library(lubridate)
library(ggplot2)
theme_set(theme_bw())
library(arrow)

L2_QAQC <- file.path(params$DATA_ROOT, params$L2_QAQC)
L2 <- file.path(params$DATA_ROOT, params$L2)

# Find all L2_qaqc files and build a table of their
# site/plot/research_name combinations
files_to_process <- list.files(L2_QAQC, 
                               pattern = "^[A-Z]{3}_[A-Z]{1,}[1-3]*_[0-9]{4}_.+(csv|parquet)$", 
                               full.names = TRUE, recursive = TRUE)
ftp_table <- tibble(file = files_to_process, filename = basename(files_to_process))
ftp_table <- separate(ftp_table, filename, 
                      into = c("site", "plot", "year", "research_name", 
                               "level", "qaqc", "version"), sep = "_", 
                      remove = FALSE)
ftp_table$level <- ftp_table$qaqc <- ftp_table$version <- NULL

QAQC_TABLE <- file.path(params$METADATA_ROOT, params$QAQC_TABLE)
qaqct <- read_csv(QAQC_TABLE, col_types = "ccccclc")
qaqct <- qaqct[!is.na(qaqct$variable),]

# Get the variables' metadata
L2_VAR_MD <- read.csv(file.path(params$METADATA_ROOT,
                                params$METADATA_VARS_TABLE))

# Get the column metadata file
column_md <- read.csv(file.path(params$METADATA_ROOT,
                                params$L2_METADATA,
                                params$METADATA_COLUMNS_TABLE),
                      comment.char = "#",
                      stringsAsFactors = FALSE)

# Read the derived variables files
derived_vars <- read_csv(file.path(params$METADATA_ROOT,
                                   params$L2_METADATA,
                                   params$DERIVED_VARS_TABLE))

source("helpers.R")
source("L2-utils.R")
source("metadata-utils.R")

vmd_combined <- combine_variable_metadata(L2_VAR_MD, derived_vars)

# Don't write plot for test data; partial workaround for
# https://github.com/COMPASS-DOE/sensor-data-pipeline/issues/415
WRITE_PLOTS <- !grepl("TEST", params$DATA_ROOT)

unlink("log.txt") # quick local progress log
```

The derived variables table has `r nrow(derived_vars)` entries.

For gap-filling, I see `r length(files_to_process)` files.

QAQC table has `r nrow(qaqct)` entries.

L2 column metadata has `r nrow(column_md)` entries.

Output directory is `r L2` (WRITE_PLOTS is `r WRITE_PLOTS`).

HTML outfile is "`r params$html_outfile`".

## Derived variables

```{r derived-vars}
errors <- 0

# Process the derived variables table row by row
for(i in seq_len(nrow(derived_vars))) {
    dv <- derived_vars$research_name[i]
    cat(as.character(Sys.time()), " L2 doing derived ", dv, "\n",
        file = "log.txt", append = TRUE)
    
    # Parse the 'needs_research_names' column and figure out which files are needed
    needs <- trimws(strsplit(derived_vars$needs_research_names[i], ",")[[1]])
    message("Calculating ", dv, " - need ", length(needs), 
            " variables: ", derived_vars$needs_research_names[i])
    files_needed <- ftp_table[ftp_table$research_name %in% needs,]
    
    # Split by site and plot and year, and then for each split...
    files_needed_split <- split(files_needed, ~ site + plot + year, drop = TRUE)
    for(grp in files_needed_split) {
        if(nrow(grp) == 0) stop("This shouldn't happen!") 
        if(nrow(grp) < length(needs)) {
            message("\tRequires ", length(needs), " variables but only ",
                    nrow(grp), "are available")
            next
        }
        
        si <- grp$site[1]
        pl <- grp$plot[1]
        yr <- grp$year[1]
        message("\tLoading data for ", si, " ", pl, " ", yr)
        # Read the files needed and put into a list, using research names as names
        dat_needed <- lapply(grp$file, function(f) {
            if(grepl("parquet$", f)) {
                read_parquet(f)
            } else if(grepl("csv$", f)) {
                read_csv(f, col_types = "Tcccccccdii")
            } else {
                stop("Don't know how to read this format")
            }
        })
        
        names(dat_needed) <- grp$research_name
        
        # We now have all the research-name data needed to compute derived variable "dv" for
        # this site and plot and year. BUT it's possible that the relevant instruments didn't
        # all measure the same thing. (This occurs at DLG, where we have TEROS12s measuring
        # three variables but TEROS21s measuring only two. Argh.)
        # So, calculate the derived variable instrument by instrument
        instruments <- unique(unlist(sapply(dat_needed, function(x) x$Instrument)))
        newdat_list <- list()
        for(ins in instruments) {
            dat_needed_ins <- lapply(dat_needed, function(x) x[x$Instrument == ins,])
            # If any of the instruments is missing needed data, not an error; just move on
            if(any(sapply(dat_needed_ins, nrow) == 0)) {
                message("\tInstrument ", ins, " is missing data; moving on")
                next
            }
            # Call CALC_DERIVED_{dv} (the calculation functions are in L2-utils.R)
            newdat_list[[ins]] <- do.call(paste0("CALC_DERIVED_", dv), list(dat_needed_ins))
        }
        newdat <- bind_rows(newdat_list)
        # Set columns to show that this is derived data
        newdat$research_name <- dv
        newdat$N_avg <- newdat$N_drop <- NA
        fqfn <- write_to_folders(newdat, 
                                 root_dir = L2, 
                                 data_level = "L2",
                                 site = si, 
                                 plot = pl, 
                                 derived_tempfile = TRUE)
        # Add this file to the files-to-process table
        # This is a one-by-one addition, not efficient, maybe TODO improve later
        new_record <- tibble(file = fqfn, filename = basename(fqfn),
                             site = si, plot = pl, year = yr,
                             research_name = dv)
        ftp_table <- bind_rows(ftp_table, new_record)
    }
    # add dv info to variables metadata table
}

```

## MAC processing

```{r mac-processing}

# The site-research name table holds the unique combinations of those attributes
# We will process all files at once that match the attributes for a given row
site_rn_table <- unique(ftp_table[c("site", "plot", "research_name")])

smry <- site_rn_table
smry$Files <- 0
smry$Dates <- ""
smry$N_vals_gf <- smry$N_vals <- 0

ftp_table$N_vals <- NA
ftp_table$N_vals_gf <- NA

for(i in seq_len(nrow(site_rn_table))) {
    si <- site_rn_table$site[i]
    pl <- site_rn_table$plot[i]
    rn <- site_rn_table$research_name[i]
    do_mac <- qaqct$do_MAC[match(rn, qaqct$variable)]
    if(is.na(do_mac)) stop("do_MAC is NA for ", rn)
    message("Processing ", si, " ", pl, " ", rn, " do_mac=", do_mac)

    # Identify files for this site, plot, and research name, and read
    these_files <- subset(ftp_table, site == si & plot == pl & research_name == rn)
    if(nrow(these_files) == 0) stop("No files! This shouldn't happen")
    smry$Files[i] <- nrow(these_files)
    
    file_data <- lapply(these_files$file, function(f) {
        if(grepl("parquet$", f)) {
            x <- read_parquet(f)
        } else if(grepl("csv$", f)) {
            x <- read_csv(f, col_types = "Tcccccccdii")
        } else {
            stop("Don't know how to read this format")
        }
        
        x$month <- month(x$TIMESTAMP)
        x$mday <- mday(x$TIMESTAMP)
        x$hour <- hour(x$TIMESTAMP)
        x$minute <- minute(x$TIMESTAMP)
        x
    })
    
    # Compute the mean annual cycle (MAC) across all sensors and files (years)
    all_data <- bind_rows(file_data)
    if(all(is.na(all_data$Value)) | !is.numeric(all_data$Value)) {
        message("\tNo non-NA numeric data in this file! Moving on")
        next
    }

    if(do_mac) {
        message("\tComputing MAC...")
        smry$N_vals[i] <- sum(!is.na(all_data$Value))
        yrs <- year(all_data$TIMESTAMP)
        smry$Dates[i] <- paste(format(min(all_data$TIMESTAMP), format = "%Y%m"),
                               format(max(all_data$TIMESTAMP), format = "%Y%m"),
                               sep = "-")
        all_data %>% group_by(month, mday, hour, minute) %>% 
            summarise(mac = mean(Value, na.rm = TRUE), .groups = "drop") %>% 
            filter(!is.nan(mac)) ->
            mac
    }
    
    # For each file's data, compute mac column and write out
    for(j in seq_len(nrow(these_files))) {
        dat <- file_data[[j]]
        if(do_mac) {
          oldrows <- nrow(dat)
          dat <- left_join(dat, mac, by = c("month", "mday", "hour", "minute"))
          stopifnot(nrow(dat) == oldrows) # sanity check

        # call gap-filling routine for each Instrument_ID and Sensor_ID
        # split doesn't work with NA values (???) so replace those...
        dat <- replace_na(dat, list(Instrument_ID = "", Sensor_ID = ""))
        # ...then split and apply
        dat_split <- split(dat, ~ Instrument_ID + Sensor_ID, drop = TRUE)
        dat_gf_list <- lapply(dat_split, function(x) {
            if(nrow(x) == 0) {
                stop("This isn't good; in ", paste(i, j, si, pl, rn))
            }
            x <- x[order(x$TIMESTAMP),]
            x$Value_MAC <- fill_all_gaps(x$Value, x$mac)
            x
        })
        dat_gf <- bind_rows(dat_gf_list)
        rownames(dat_gf) <- NULL
        # Transfer good values into gap-fill column
        nagf <- is.na(dat_gf$Value_MAC)
        dat_gf$Value_MAC[nagf] <- dat_gf$Value[nagf]
        
        # MAC values are not allowed to drift outside of
        # the limits defined in variables_metadata.csv
        rni <- which(rn == vmd_combined$research_name)
        if(length(rni) == 1) { # i.e., if research name exists in table
            lb <- vmd_combined$low_bound[rni]
            if(!is.na(lb)) { # if a low bound is defined, truncate to it
                dat_gf$Value_MAC[dat_gf$Value_MAC < lb] <- lb
            }
            hb <- vmd_combined$high_bound[rni]
            if(!is.na(hb)) { # if a high bound is defined, truncate to it
                dat_gf$Value_MAC[dat_gf$Value_MAC > hb] <- hb
            }
        }
        
        message("\tGap filled ", 
                sum(is.na(dat_gf$Value)) - sum(is.na(dat_gf$Value_MAC)),
                " values in ", these_files$filename[j])
        # Update the summary and files-to-process tables
        f <- which(these_files$filename[j] == ftp_table$filename)
        ftp_table$N_vals[f] <- sum(!is.na(dat_gf$Value))
        ftp_table$N_vals_gf[f] <- sum(!is.na(dat_gf$Value_MAC))
        smry$N_vals_gf[i] <- smry$N_vals_gf[i] + sum(!is.na(dat_gf$Value_MAC))
        } else {
            dat_gf <- dat
            dat_gf$Value_MAC <- NA
            smry$N_vals_gf[i] <- 0
        }
        
        # Clean up and check that the data frame has correct columns
        dat_gf <- check_drop_sort_columns(dat_gf, column_md, params$METADATA_COLUMNS_TABLE)

        write_to_folders(dat_gf,
                         root_dir = L2, 
                         data_level = "L2",
                         site = si,
                         plot = pl,
                         variable_metadata = vmd_combined,
                         version = params$L2_VERSION,
                         write_plots = WRITE_PLOTS) 
    }
    
}
```

## Summary

```{r summary}
#| echo: false
#| output: asis
if(errors) {
    cat("### WARNING: ", errors, " file read/write error(s)\n")
    log_warning(paste("File read/write error(s)", params$html_outfile), 
                logfile = params$logfile)
}
```

```{r summary_table}
knitr::kable(smry)
```

## Metadata

L2 metadata template directory is `r params$L2_METADATA`.

```{r metadata}

source("metadata-utils.R")

# Write the overall README
readme_fn <- file.path(params$METADATA_ROOT,
                       params$L2_METADATA,
                       params$RELEASE_README_FILES, 
                       paste0("README_v", params$L2_VERSION, ".txt"))

n_obs <- format(sum(smry$N_vals), big.mark = ",")
readme <- md_readme_substitutions(readme_fn,
                                  params$L2_VERSION, 
                                  params$L2_RELEASE_DATE, 
                                  n_obs,
                                  GIT_COMMIT,
                                  params$L2_DATA_TIMEZONE)
readme_outfn <- file.path(L2, basename(readme_fn))
message("Writing overall README ", readme_outfn, "...")
writeLines(readme, readme_outfn)

# Get the L2 template file
template_file <- file.path(params$METADATA_ROOT,
                           params$L2_METADATA, 
                           "L2_metadata_template.txt")
if(!file.exists(template_file)) {
    stop("Can't find file ", basename(template_file), " in ", params$L2_METADATA)
}
L2_metadata_template <- readLines(template_file)

col_md_for_insert <- paste(sprintf("%-15s", column_md$Column), column_md$Description)

# Format the variable metadata
var_md_for_insert <- md_variable_info(vmd_combined, 
                                      L1_or_L2 = "L2",
                                      only_these = qaqct$variable)

message("Main template has ", length(L2_metadata_template), " lines")
message("Column metadata info has ", length(col_md_for_insert), " lines")
message("Variable metadata info has ", length(var_md_for_insert), " lines")

# Identify the main data directories in L2, which are <site>_<year>
data_dirs <- list.files(L2, pattern = "^[a-zA-Z]+_[0-9]{4}$")
site_files_folder <- file.path(params$METADATA_ROOT, 
                               params$METADATA_SITE_FILES)

for(dd in data_dirs) {
    dd_full <- file.path(L2, dd)
    message("Generating metadata for ", dd_full)
    
    message("\tInserting timestamp and folder name")
    md <- gsub("[TIMESTAMP]", date(), L2_metadata_template, fixed = TRUE)
    md <- gsub("[FOLDER_NAME]", dd, md, fixed = TRUE)
    
    # Insert info on data files into metadata
    md <- md_insert_fileinfo(dd_full, md, level = "L2")
    
    # Insert column metadata
    col_info_pos <- grep("[COLUMN_INFO]", md, fixed = TRUE)
    md <- append(md, col_md_for_insert, after = col_info_pos)
    md <- md[-col_info_pos]
    
    # Insert NA code, time zone, and version information
    md <- md_insert_miscellany(md, 
                               NA_STRING_L2, 
                               params$L2_DATA_TIMEZONE, 
                               params$L2_VERSION)
    
    # Insert variable metadata
    var_info_pos <- grep("[VARIABLE_INFO]", md, fixed = TRUE)
    md <- append(md, var_md_for_insert, after = var_info_pos)
    md <- md[-var_info_pos]
    
    # Site information
    # Folders are <site>_<year>
    site <- strsplit(dd, "_")[[1]][1]
    md <- md_insert_siteinfo(site, site_files_folder, md)

    # QAQC information
    interp_for_insert <- paste(sprintf("%-20s", c("Variable", qaqct$variable)),
                           sprintf("%-15s", c("Max_interpolation", qaqct$max_interp)),
                           sprintf("%-20s", c("Notes", qaqct$Notes)))
    interp_for_insert <- gsub("NA", "--", interp_for_insert) # looks nicer
    interp_info_pos <- grep("[INTERP_INFO]", md, fixed = TRUE)
    md <- append(md, interp_for_insert, after = interp_info_pos)
    md <- md[-interp_info_pos]

    # Write the final metadata file
    mdfn <- paste0(dd, "_L2_v", params$L2_VERSION, "_metadata.txt")
    message("\tWriting ", mdfn, "...")
    writeLines(md, file.path(L2, dd, mdfn))
}
```

## Documentation and code

```{r docs-and-code}
ok <- file.copy(params$CODE_EXAMPLES, L2, recursive = TRUE)
if(ok) {
    message("Copied ", params$CODE_EXAMPLES, " to ", L2)
} else {
    warning("There was a problem copying ", params$CODE_EXAMPLES)
}
```

## Reproducibility

Git commit `r GIT_COMMIT`.

```{r reproducibility}
sessionInfo()
```
