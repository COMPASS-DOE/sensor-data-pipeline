---
title: "L2"
author: "COMPASS workflows team"
title-block-banner: true
params:
  html_outfile: "L2.html"
  DATA_ROOT: "data_TEST/"
  L1: "L1/"
  L2: "L2/"
  QAQC_TABLE: "qaqc_table.csv"
  timestamp_round: "15 min"
  # 'maxgap' controls zoo::na_approx's gap-filling behavior
  # It's in units of `timestamp_round` above
  # By default we only interpolate fill <= 1 hour gaps
  maxgap: 4
  METADATA_ROOT: "metadata/"
  L2_METADATA: "L2_metadata/"
  # We use "Etc/GMT+5" rather than e.g. "America/New_York" for
  # L1_DATA_TIMEZONE because outputs should always be in STANDARD time
  # See https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
  L1_DATA_TIMEZONE: "Etc/GMT+5"
  L2_VERSION: "???"
  L2_RELEASE_DATE: "???"
  write_plots: true
  logfile: ""
date: now
date-format: "YYYY-MM-DD HH:mm:ssZ"
format: 
  html:
    embed-resources: true
    code-fold: true
editor: visual
---

This script

-   Reads in the L1 data files one by one

-   Drops out of bounds (`F_OOB`) and out of service (`F_OOS`) rows

-   Drop NA data rows (shouldn't be any?)

-   Identifies and drops outliers

-   Averages to `r params$timestamp_round`, recording n_good and n_outliers

-   Fills in (NA) missing timestamps for the year

-   Computes new variables

-   Write L2 files

## Initializing

```{r init}
#| include: false

library(tidyr)
library(readr)
library(lubridate)
library(zoo)
library(ggplot2)

L1 <- file.path(params$DATA_ROOT, params$L1)
L2 <- file.path(params$DATA_ROOT, params$L2)

files_to_process <- list.files(L1, 
                               pattern = "[A-Z]{3}_[A-Z]{1,2}_[0-9]{8}-[0-9]{8}_.+csv$", 
                               full.names = TRUE, recursive = TRUE)

# **** TEMPORARY **** TODO
files_to_process <- list.files(L1, 
                               pattern = "SWH_SWAMP_20241130-[0-9]{8}_.+csv$", 
                               full.names = TRUE, recursive = TRUE)

QAQC_TABLE <- file.path(params$METADATA_ROOT, params$QAQC_TABLE)
qaqct <- read_csv(QAQC_TABLE, col_types = "ccccc")

source("helpers.R")
source("L2-utils.R")
```

I see `r length(files_to_process)` files to process in `r L1`.

QAQC table `r QAQC_TABLE` and has `r nrow(qaqct)` entries.

Round data to `r params$timestamp_round` and fill to max gap size of `r params$maxgap`.

Output directory is `r L2`

HTML outfile is "`r params$html_outfile`".

## Processing

```{r processing}
overwrites <- 0
errors <- 0
dat_issues <- 0

f <- function(fn, out_dir) {
    message(Sys.time(), " Processing ", basename(fn))

    dat <- read_csv(fn, col_types = "ccTccccdccii")
    message("\tTotal data: ", nrow(dat), " rows, ", ncol(dat), " columns")

    # TODO: there might be more than one instrument per file (battery voltage)
    
    rn <- unique(dat$research_name)
    if(length(rn) > 1) {
        stop("Why is there more than one research name here?!?")
    }
    i <- match(rn, qaqct$variable)
    if(is.na(i)) stop(rn, " not found in QAQC table")
    if(length(i) > 1) stop(rn, " found in more than one row of QAQC table")
    
    # TODO: loop through the QAQC_ columns in the table
    
    if(is.na(qaqct$QAQC_mad[i])) {
        message("\tNo QAQC requested")
        dat$F_mad <- FALSE
    } else {
        message("\tRunning QAQC_mad")
        if(!is.na(qaqct$params[i])) message("\t\t", qaqct$params[i])
        otherparams <- qaqct$params[i]
        dat <- do_outlier_test(dat, 
                               "QAQC_mad", 
                               time_grouping = qaqct$time_grouping[i], 
                               otherparams = otherparams)
        if(sum(dat$F_mad)) {
            message("\t\t", sum(dat$F_mad), " outliers identified")
        }
        p <- ggplot(dat, aes(TIMESTAMP, Value, color = F_mad)) + 
            geom_point(na.rm = TRUE) + ggtitle(basename(fn), 
                                               subtitle = paste(qaqct$time_grouping[i], otherparams))
        print(p)
    }
    
    # File-based summary
    smry <- data.frame(File = basename(fn), 
                       Site = dat$Site[1],
                       Plot = dat$Plot[1],
                       research_name = dat$research_name[1],
                       Year = unique(year(dat$TIMESTAMP)),
                       Rows = nrow(dat))
    
    # loop through all the F_ flags, count, and add to summary table,
    flag_cols <- grep("^F_", colnames(dat))
    dat$F_drop <- rep(0L, nrow(dat)) # signals whether to drop row or not
    for(fname in colnames(dat)[flag_cols]) {
        # OOB and OOS flags might be NA
        dat[[fname]][is.na(dat[[fname]])] <- FALSE
        
        f_smry <- data.frame(sum(dat[[fname]], na.rm = TRUE))
        colnames(f_smry) <- fname
        smry <- cbind(smry, f_smry)
        
        dat$F_drop <- dat$F_drop + dat[[fname]]
    }
    smry$Total <- sum(dat$F_drop)
    smry$Pct <- paste0(sprintf("%6.1f", smry$Total / nrow(dat) * 100), "%")
    
    # Round timestamps and average
    dat$TIMESTAMP <- round_date(dat$TIMESTAMP, unit = params$timestamp_round)
    
    # This would be easier and faster to do in dplyr or data.table but
    # I'm trying to minimize dependencies; so... 
    dat$Value[as.logical(dat$F_drop)] <- NA 
    
    # aggregate() chokes on NAs in grouping variables
    dat <- replace_na(dat, list(Instrument_ID = "", Sensor_ID = "", Location = ""))
    
    dat_summarised <- aggregate(Value ~ Site + Plot + TIMESTAMP + Instrument +
                                    Instrument_ID + Sensor_ID + Location, 
                                data = dat, 
                                FUN = mean, na.action = na.omit, drop = FALSE)
    # Summarise number of used and not used values
    dat$F_keep <- !dat$F_drop
    dat2 <- aggregate(F_keep ~ Site + Plot + TIMESTAMP + Instrument +
                          Instrument_ID + Sensor_ID + Location, 
                      data = dat, 
                      FUN = sum, na.action = na.omit, drop = FALSE)
    
    dat3 <- aggregate(F_drop ~ Site + Plot + TIMESTAMP + Instrument +
                          Instrument_ID + Sensor_ID + Location, 
                      data = dat, 
                      FUN = sum, na.action = na.omit, drop = FALSE)
    dat_summarised$n <- dat2$F_keep
    dat_summarised$n_drop <- dat3$F_drop
    
    # Complete the data: fill in (with NA) all timestamps and all combinations
    # of Instrument, Sensor, etc. This guarantees a smooth time series for
    # each sensor
    rows <- nrow(dat_summarised)
    dat_summarised <- L2_complete(dat_summarised) # function is in L2-utils
    message("\tCompleted data; started with ", rows, 
            " rows...now ", nrow(dat_summarised))
    
    # Gap fill for each site, plot, instrument, and sensor
    na_vals <- is.na(dat_summarised$Value)
    dat_summarised <- L2_gapfill(dat_summarised, params$maxgap)
    new_na_vals <- is.na(dat_summarised$Value)
    message("\tGap-filled data; started with ", sum(na_vals), 
            " missing values...now ", sum(new_na_vals))
    dat_summarised$F_GAP <- na_vals & !new_na_vals

    p <- ggplot(dat_summarised, aes(TIMESTAMP, Value, color = F_GAP)) + 
        geom_point(na.rm = TRUE) + ggtitle(basename(fn))
    print(p)
    
    message("\tWriting...")
    newfn <- file.path(params$DATA_ROOT, params$L2, basename(fn))
    newfn <- gsub("_L1_", "_L2_", newfn, fixed = TRUE)
    write_csv(dat_summarised, newfn, na = "")
    
    return(smry)
}

log_info("About to L2", logfile = params$logfile)
tryCatch({
    out <- lapply(files_to_process, f, out_dir = L2)
},
error = function(e) {
    log_warning("L2: an error occurred!", logfile = params$logfile)
    log_info(as.character(e), logfile = params$logfile)
    stop(e)
})
```

## Summary

```{r summary}
#| echo: false
#| output: asis
if(errors) {
    cat("### WARNING: ", errors, " file read/write error(s)\n")
    log_warning(paste("File read/write error(s)", params$html_outfile), 
                logfile = params$logfile)
}
```

```{r summary_table}
out_df <- do.call("rbind", out)
knitr::kable(out_df[-1]) # don't show filename
```

## Reproducibility

Git commit `r GIT_COMMIT`.

```{r reproducibility}
sessionInfo()
```
